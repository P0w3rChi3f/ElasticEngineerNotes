# Logstash

## Logstash Install
```
yum install Logstash
```

## Options Logstash Configs
```
vi /etc/Logstash/jvm.options
6 -Xms1g
7 -Xmx1g
```

```
vi /etc/Logstash/Logstash.yml
```

## Pipeline Configuration
```
cd /etc/logstash/conf.d/
vi 100-input-zeek.conf
```

### 100-input-zeek.conf
```
input {
  kafka{
    add_field => { "[@metadata][stage]" => "zeek-raw" }
    topics => ["zeek-raw"]
    bootstrap_servers => "172.16.90.100:9092"
    # set this to one per kafka partition to scale up 
    # consumer_threads => 4
    group_id => "bro_logstash"
    codec => json
    auto_offset_reset => "earliest"
  }
}
```
### 500-filter-zeek.conf
```
cd /etc/logstash//conf.d/
vi 500-filter-zeek.conf
```

```
filter {


}
```
### Config 999-output-zeek.conf
```
cd /etc/logstash/conf.d/
vi 999-output-zeek.conf
```
```
output{
    if[@metadata][stage] == "zeek-raw" {
        elasticsearch {
            hosts => ["172.16.90.100:9200"]
            index => "zeek-%{+YYYY.MM.dd}"
            template => "/etc/logstash/bro-index-template.json"
        
        }
    }
}
```

## Test the status of logstash
```
systemctl start logstash
systemctl status logstash
tail -f /var/log/logstash/logstash-plain.log
curl 172.16.90.100:9200
/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/999-output-zeek.conf -t
```

## Config Zeek Index Templates
Create the index pattern
```
http://172.16.90.100:5601/ -> Settings -> Management -> index patterns -> zeek-* -> @timestamp
```
Copy the config
```
Index management - click index zeek-2020.01.14 - mappings - copy everything into clipboard
```
[Online index template](https://github.com/rocknsm/rock-dashboards/blob/master/configuration/elasticsearch/bro_index.json)

Edit the Template.json File
```
sudo vi /etc/logstash/bro-index-template.json
```
```
"id": {
    "properties": {  
        "orig_h": {  
        "type": "ip",  
        ~~~"fields": {~~~  
        ~~~ "keyword": {~~~  
        ~~~"type": "keyword",~~~  
        ~~~"ignore_above": 256~~~  
        ~~~}~~~  
                }  
            }
        },  
        "resp_h": {  
            "type": "ip",  
            ~~~ "fields": { ~~~ 
            ~~~"keyword": { ~~~ 
            ~~~"type": "keyword", ~~~ 
            ~~~"ignore_above": 256 ~~~
           ~~~ }  ~~~
        ~~~} ~~~ 
        },  
```
Restart Logstash
```
sudo systemctl stop logstash
```
Prep for new Index
```
Kibana -> index management -> select index -> manage index -> delete index

sudo vi /etc/logstash/conf.d/999-output-zeek.conf

6 # template => "/etc/logstash/bro-index-template.json"
```
Restart Logstash
```
sudo systemctl start logstash
Kibana - index management - Ctrl+F: orig_h
```

```
sudo systemctl stop logstash
Kibana -> index management -> select index -> manage index -> delete index
```

Configure Index Mappings
```
Kibana -> Dev Tools
PUT _template/zeek_index_mappings
{
  "order": 10,
  "index_patterns": [
      "zeek-*",
  ],
  "mappings": {
...
      "id": {
    "properties": {  
        "orig_h": {  
        "type": "ip",  
        ~~~"fields": {~~~  
        ~~~ "keyword": {~~~  
        ~~~"type": "keyword",~~~  
        ~~~"ignore_above": 256~~~  
        ~~~}~~~  
                }  
            }
        },  
        "resp_h": {  
            "type": "ip",  
            ~~~ "fields": { ~~~ 
            ~~~"keyword": { ~~~ 
            ~~~"type": "keyword", ~~~ 
            ~~~"ignore_above": 256 ~~~
           ~~~ }  ~~~
        ~~~} ~~~ 
        },
...

```

#### Test Index File
```
GET _cat/templates
GET _template/zeek_index_mappings
```

## Edit the 500-filter-zeek.conf
```
cd /etc/logstash//conf.d/
vi 500-filter-zeek.conf
```
```
filter {
    if [@metadata][stage] == "zeek_raw" {
        mutate {
            add_field => {"processed_time" => "@timestamp"}
        }
        date { match => [ "ts", "ISO8601" ] }
        mutate {
            add_field => { "orig_host" => "%{id.orig_h}" }
            add_field => { "resp_host" => "%{id.resp_h}" }
            add_field => { "src_ip" => "%{id.orig_h}" }
            add_field => { "dst_ip" => "%{id.resp_h}" }
            add_field => { "related_ips" => [] }
         }
         mutate {
            merge => { "related_ips" => "id.orig_h" }
         }
        mutate {
            merge => { "related_ips" => "id.resp_h" }
        }
    }

}
```
Restart logstash
```
systemctl restart logstash
systemctl status logstash
```
## FSF data into Elasic search

### 100-input-zeek.conf
```
input {
  kafka{
    add_field => { "[@metadata][stage]" => "fsf-raw" }
    topics => ["fsf-raw"]
    bootstrap_servers => "172.16.90.100:9092"
    # set this to one per kafka partition to scale up 
    # consumer_threads => 4
    group_id => "fsf_logstash"
    codec => json
    auto_offset_reset => "earliest"
  }
}
```

### Edit the 500-filter-fsf.conf
```
cd /etc/logstash//conf.d/
vi 500-filter-fsf.conf
```
```
filter {
    if [@metadata][stage] == "fsf-raw" {
        mutate {
            add_field => {"processed_time" => "@timestamp"}
        }
        date { match => [ "ts", "ISO8601" ] }
        mutate {
            add_field => { "orig_host" => "%{id.orig_h}" }
            add_field => { "resp_host" => "%{id.resp_h}" }
            add_field => { "src_ip" => "%{id.orig_h}" }
            add_field => { "dst_ip" => "%{id.resp_h}" }
            add_field => { "related_ips" => [] }
         }
         mutate {
            merge => { "related_ips" => "id.orig_h" }
         }
        mutate {
            merge => { "related_ips" => "id.resp_h" }
        }
    }

}
```
### 999-output-fsf.conf
```
cd /etc/logstash/conf.d/
vi 999-output-fsf.conf
```
```
output{
    if[@metadata][stage] == "fsf-raw" {
        elasticsearch {
            hosts => ["172.16.90.100:9200"]
            index => "fsf-%{+YYYY.MM.dd}"
        }
    }
}
```

## Suricata data into Elasic search

### 100-input-Suricata.conf
```
input {
  kafka{
    add_field => { "[@metadata][stage]" => "suricata-raw" }
    topics => ["suricata-raw"]
    bootstrap_servers => "172.16.90.100:9092"
    # set this to one per kafka partition to scale up 
    # consumer_threads => 4
    group_id => "suricata-raw_logstash"
    codec => json
    auto_offset_reset => "earliest"
  }
}
```

### Edit the 500-filter-fsf.conf
```
cd /etc/logstash//conf.d/
vi 500-filter-fsf.conf
```
```
filter {
    if [@metadata][stage] == "fsf-raw" {
        mutate {
            add_field => {"processed_time" => "@timestamp"}
        }
        date { match => [ "ts", "ISO8601" ] }
        mutate {
            add_field => { "orig_host" => "%{id.orig_h}" }
            add_field => { "resp_host" => "%{id.resp_h}" }
            add_field => { "src_ip" => "%{id.orig_h}" }
            add_field => { "dst_ip" => "%{id.resp_h}" }
            add_field => { "related_ips" => [] }
         }
         mutate {
            merge => { "related_ips" => "id.orig_h" }
         }
        mutate {
            merge => { "related_ips" => "id.resp_h" }
        }
    }

}
```
### 999-output-fsf.conf
```
cd /etc/logstash/conf.d/
vi 999-output-suricata.conf
```
```
output{
    if[@metadata][stage] == "suricata-raw" {
        elasticsearch {
            hosts => ["172.16.90.100:9200"]
            index => "suricata-%{+YYYY.MM.dd}"
        }
    }
}
```

## Prepair for production
```
systemctl stop logstash suricata stenographer fsf kafka zookeeper elasitcsearch
```
```
rm -rf /data/suricata/*
rm -rf /var/lib/zookeeper/version-2/
rm -rf /data/kafka/*
rm -rf /data/fsf/logs/rockout.log
rm -rf /data/suricata/eve.json
rm -rf /steno/thread0/packets/*
rm -rm /steno/thread0/index/*
```
Restart Everything
```
systemctl start elasticsearch
systemctl start suricata zookeeper stenographer kafka logstash fsf
```
